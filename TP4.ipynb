{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faouzi-Slimi/Text_Mining__NLP/blob/main/TP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37Kx1-7Or-ki"
      },
      "source": [
        "# Topic Model\n",
        "Dans ce TP, nous allons, dans un premier lieu, préparer le corpus de documents extrait du Corpus NLTK 'brown', d'extraire les termes descripteurs en passant par les étapes permettant d'améliorer la qualité de ces descripteurs. Par la suite la matrice terme/documents sera construite.\n",
        "\n",
        "Dans un second lieu, un changement de projection des documents dans une nouvelle représentation sera appliqué en utilisant la méthode LDA. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEfcTcfcvvHJ"
      },
      "source": [
        "Nous commonçons tout d'abord par un petit exemple pour comprendre le principe. soit la variable documents suivante contenant 9 documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg3fT6Lr4D6G"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import scipy.sparse\n",
        "from gensim import corpora\n",
        "from gensim import matutils, models\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GexUcpjgv4m6"
      },
      "source": [
        " documents = [\"Human machine interface for lab abc computer applications human human machine\",\n",
        "              \"A survey of user opinion of computer system response time\",\n",
        "              \"The EPS user interface management system\",\n",
        "              \"System and human system engineering testing of EPS\",\n",
        "              \"Relation of user perceived response time to error measurement\",\n",
        "              \"The generation of random binary unordered trees\",\n",
        "              \"The intersection graph of paths in trees\",\n",
        "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "              \"Graph minors A survey\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vU3mCjL2UJz"
      },
      "source": [
        "Nous cherchons par la suite le vocabulaire. La fonction **CountVectorizer()** retourne les indexes des tokens ainsi que leurs nombres d'apparition. Si vous ne fournissez pas un ensemble de termes descripteurs ou un analyzer elle retournera la totalité des termes du vocabulaire comme étant les termes descripteurs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v7IM22X3B1t"
      },
      "source": [
        "cv = CountVectorizer()\n",
        "data_cv = cv.fit_transform(documents)\n",
        "print(cv.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_4EvNmc0IAm"
      },
      "source": [
        "Dans la suite nous utilisons ces termes descripteurs pour construire le dataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueYEtOxDrtDE"
      },
      "source": [
        "cv_df = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "cv_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKZBgdVsKitB"
      },
      "source": [
        "Changement du type dataFrame vers le type de **Gensim** et construction du dictionnaire *id2word* (information fournit par le CountVectorizer())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZkdU1ujKWbq"
      },
      "source": [
        "cp = cv_df.transpose()\n",
        "sparse_counts = scipy.sparse.csr_matrix(cp)\n",
        "corpus_brown = matutils.Sparse2Corpus(sparse_counts)\n",
        "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-GupNrJMA7F"
      },
      "source": [
        "Nous pouvons maintenant trouver une nouvelle représentation du corpus en utilisant Latent Dirichlet Allocation, LDA en utilisant  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxgGdKaDMz0i"
      },
      "source": [
        "lda = models.LdaModel(corpus=corpus_brown, id2word=id2word, num_topics=2, passes=150)\n",
        "lda.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVIZffcLM3sG"
      },
      "source": [
        "# Partie II\n",
        "Objectif: Appliquer la même démarche sur un corpus plus évoluer avec une sélection des descripteurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAcVXsFMQM3M"
      },
      "source": [
        "1) Construire le corpus des données dans un dictionnaire \n",
        "\n",
        "corpus  {doc1:text1,doc2:text2,...}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjsBPGMkM3AW"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "corpus = {}\n",
        "for fileid in ['cn02','cn01','cn03','cd01','cd02']:\n",
        "  corpus[fileid] = ' '.join(brown.words(fileids=fileid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWL1Ql9nQxWM"
      },
      "source": [
        "2) Changer le dictionnaire en DataFrame en colonne le noms des documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AWNKsanRXvZ"
      },
      "source": [
        "pd.DataFrame([corpus.values()],columns=['cn02','cn01','cn03','cd01','cd02'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om5_8p5SRd0r"
      },
      "source": [
        "3) Appliquer le transposer et nommer la colonne du text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_wmNrwARdFc"
      },
      "source": [
        "data_t=data.transpose()\n",
        "data_t.columns = ['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Q_MMrWR5o2"
      },
      "source": [
        "4) Developper une fonction qui permet d'éliminer les stopwords, éliminer la ponctuation, appliquer un lematization sur les mots. \n",
        "\n",
        "Cet fonction sera le paramétre d'entree de l'analyzer de l'objet CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbrsLVJpS3r8"
      },
      "source": [
        "cv = CountVectorizer(analyzer=clean_data)\n",
        "data_cv = cv.fit_transform(data_t.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjEj4YIdTXOP"
      },
      "source": [
        "5) Appliquer sur la matrice documents/termes le LDAModel() comme présenté dans la première partie"
      ]
    }
  ]
}